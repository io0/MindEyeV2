{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541c729-b303-4f1f-a590-7a5107f8a401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# # SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import Clipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "[2024-01-18 12:52:49,763] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'GLOBAL_BATCH_SIZE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLOBAL_BATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(global_batch_size) \u001b[38;5;66;03m# set this to your batch size!\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     global_batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGLOBAL_BATCH_SIZE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m    \n\u001b[1;32m     33\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLOBAL_BATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_devices\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_devices \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GLOBAL_BATCH_SIZE'"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "# ## IF NOT USING DEEPSPEED ###\n",
    "# use_deepspeed = False\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "global_batch_size = batch_size = 16\n",
    "\n",
    "### DEEPSPEED INITIALIZATION ###\n",
    "# use_deepspeed = True\n",
    "# import deepspeed\n",
    "# num_devices = torch.cuda.device_count()\n",
    "# if num_devices==0: num_devices = 1\n",
    "# if num_devices <= 1 and utils.is_interactive():\n",
    "#     global_batch_size = batch_size = 32\n",
    "#     print(f\"Setting batch_size to {batch_size}\")\n",
    "#     # can emulate a distributed environment for deepspeed to work in jupyter notebook\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = str(np.random.randint(10000)+9000)\n",
    "#     os.environ[\"RANK\"] = \"0\"\n",
    "#     os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "#     os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "#     os.environ[\"GLOBAL_BATCH_SIZE\"] = str(global_batch_size) # set this to your batch size!\n",
    "# else:\n",
    "#     global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]    \n",
    "#     batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices\n",
    "#     if num_devices <= 1:\n",
    "#         os.environ[\"RANK\"] = \"0\"\n",
    "#         os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "#         os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# # alter the deepspeed config according to your global and local batch size\n",
    "# if local_rank == 0:\n",
    "#     with open('deepspeed_config_stage2_cpuoffload.json', 'r') as file:\n",
    "#         config = json.load(file)\n",
    "#     config['train_batch_size'] = int(os.environ[\"GLOBAL_BATCH_SIZE\"])\n",
    "#     config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "#     config['bf16'] = {'enabled': False}\n",
    "#     config['fp16'] = {'enabled': True}\n",
    "#     with open('deepspeed_config_stage2_cpuoffload.json', 'w') as file:\n",
    "#         json.dump(config, file)\n",
    "# else:\n",
    "#     # give some time for the local_rank=0 gpu to prep new deepspeed config file\n",
    "#     time.sleep(10)\n",
    "# deepspeed_plugin = DeepSpeedPlugin(\"deepspeed_config_stage2_cpuoffload.json\")\n",
    "# accelerator = Accelerator(split_batches=False, deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 3297442\n",
      "device: cuda:0\n",
      "Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'zero_optimization': {'stage': 2, 'contiguous_gradients': True, 'stage3_gather_16bit_weights_on_model_save': True, 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_prefetch_bucket_size': 10000000.0, 'stage3_param_persistence_threshold': 100000.0, 'reduce_bucket_size': 10000000.0, 'sub_group_size': 1000000000.0, 'offload_optimizer': {'device': 'cpu', 'nvme_path': '/scratch', 'pin_memory': True}, 'offload_param': {'device': 'none', 'nvme_path': '/scratch', 'buffer_size': 4000000000.0, 'pin_memory': True}}, 'aio': {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'train_batch_size': 32, 'train_micro_batch_size_per_gpu': 32, 'wall_clock_breakdown': False, 'zero_allow_untested_optimizer': True}\n",
      "\n",
      "distributed = True num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: meV2_ablation\n",
      "--data_path=/weka/proj-fmri/shared/mindeyev2_dataset                     --model_name=meV2_ablation                     --no-multi_subject --subj=1 --batch_size=32 --num_sessions=3                     --hidden_dim=4096 --clip_scale=1.                     --blurry_recon --blur_scale=.5                      --seq_past=0 --seq_future=0                     --use_prior --prior_scale=30 --no-visualize_prior                     --use_git --git_scale=3                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=16 --no-use_image_aug                     --ckpt_interval=1 --no-ckpt_saving --no-wandb_log\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"meV2_ablation\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} --num_sessions=3 \\\n",
    "                    --hidden_dim=4096 --clip_scale=1. \\\n",
    "                    --blurry_recon --blur_scale=.5  \\\n",
    "                    --seq_past=0 --seq_future=0 \\\n",
    "                    --use_prior --prior_scale=30 --no-visualize_prior \\\n",
    "                    --use_git --git_scale=3 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=16 --no-use_image_aug \\\n",
    "                    --ckpt_interval=1 --no-ckpt_saving --no-wandb_log\"# \\\n",
    "                    # --multisubject_ckpt=../train_logs/multisubject_150ep_4block_hid4096_bs64\" \n",
    "    # --resume_from_ckpt \n",
    "    # --multisubject_ckpt=../train_logs/multisubject_150ep_4block_hid4096_bs64 \\\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 3\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_git\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) for GIT model using CLIP ViT-L/14\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--git_scale\",type=float,default=3,\n",
    "    help=\"multiply git diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=0,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 32 num_iterations_per_epoch = 70 num_samples_per_epoch = 2250\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 3 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 1 seq_len (chose 0 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "    print(train_url)\n",
    "    \n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # Load hdf5 data for betas, but don't put everything into memory\n",
    "    f = h5py.File(f'/weka/proj-fmri/shared/mindeyev2_dataset/betas_all_subj0{s}_fp32_renorm.hdf5', 'r')\n",
    "    \n",
    "    betas = f['betas'][:]\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "\n",
    "clip_seq_dim = 257\n",
    "clip_emb_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    # autoenc = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/weka/proj-fmri/shared/cache\")\n",
    "    # autoenc = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16, cache_dir=\"/weka/proj-fmri/shared/cache\")\n",
    "    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load('/weka/proj-fmri/shared/cache/sd_var_enc/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "    from convnext import ConvnextXL\n",
    "    cnx = ConvnextXL('/weka/proj-fmri/shared/cache/convnextv2/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(device)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "    \n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "64,409,600 total\n",
      "64,409,600 trainable\n",
      "param counts:\n",
      "64,409,600 total\n",
      "64,409,600 trainable\n",
      "torch.Size([2, 1, 15724]) torch.Size([2, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "962,826,366 total\n",
      "962,826,366 trainable\n",
      "param counts:\n",
      "1,027,235,966 total\n",
      "1,027,235,966 trainable\n",
      "b.shape torch.Size([2, 1, 4096])\n",
      "torch.Size([2, 257, 768]) torch.Size([2, 257, 768]) torch.Size([2, 4, 28, 28]) torch.Size([2, 49, 512]) torch.Size([2, 257, 1024])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, h=4096, in_dim=15724, out_dim=768, seq_len=2, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.backbone_linear = nn.Linear(h * seq_len, out_dim, bias=True) \n",
    "        self.clip_proj = self.projector(clip_size, clip_size, h=clip_size)\n",
    "        self.git_proj1 = nn.Linear(257, 257)\n",
    "        self.git_proj2 = self.projector(clip_size, 1024, h=1024)\n",
    "        \n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Linear(h*seq_len,4*28*28,bias=True)\n",
    "            self.bdropout = nn.Dropout(.3)\n",
    "            self.bnorm = nn.GroupNorm(1, 64)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=64,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            self.b_maps_projector = nn.Sequential(\n",
    "                nn.Conv2d(64, 512, 1, bias=False),\n",
    "                nn.GroupNorm(1,512),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(512, 512, 1, bias=False),\n",
    "                nn.GroupNorm(1,512),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(512, 512, 1, bias=True),\n",
    "            )\n",
    "            \n",
    "    def projector(self, in_dim, out_dim, h=2048):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors\n",
    "        c,b,t = torch.Tensor([0.]), torch.Tensor([[0.],[0.]]), torch.Tensor([0.])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.backbone_linear(x).reshape(len(x), -1, self.clip_size)\n",
    "        c = self.clip_proj(backbone)\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = self.bdropout(b)\n",
    "            b = b.reshape(b.shape[0], -1, 7, 7).contiguous()\n",
    "            b = self.bnorm(b)\n",
    "            b_aux = self.b_maps_projector(b).flatten(2).permute(0,2,1)\n",
    "            b_aux = b_aux.view(len(b_aux), 49, 512)\n",
    "            b = (self.bupsampler(b), b_aux)\n",
    "            \n",
    "        if use_git:\n",
    "            t = self.git_proj1(backbone.permute(0,2,1))\n",
    "            t = self.git_proj2(t.permute(0,2,1))\n",
    "        \n",
    "        return backbone, c, b, t\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_, text_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape, text_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc3af7f-bd26-4724-8eb3-686e83ba15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "56,055,184 total\n",
      "56,055,168 trainable\n",
      "param counts:\n",
      "1,083,291,150 total\n",
      "1,083,291,134 trainable\n",
      "Creating versatile diffusion reconstruction pipeline...\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 64\n",
    "    heads = clip_emb_dim//64 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        vd_cache_dir = \"/weka/proj-fmri/shared/cache/versatile-diffusion/\"\n",
    "        print('Creating versatile diffusion reconstruction pipeline...')\n",
    "        from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "        from diffusers.models import DualTransformer2DModel\n",
    "        try:\n",
    "            vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(vd_cache_dir).to('cpu')\n",
    "        except:\n",
    "            print(\"Downloading Versatile Diffusion to\", vd_cache_dir)\n",
    "            vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "                    \"shi-labs/versatile-diffusion\",\n",
    "                    cache_dir = vd_cache_dir).to('cpu')\n",
    "        vd_pipe.image_unet.eval()\n",
    "        vd_pipe.vae.eval()\n",
    "        vd_pipe.image_unet.requires_grad_(False)\n",
    "        vd_pipe.vae.requires_grad_(False)\n",
    "    \n",
    "        vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "        num_inference_steps = 20\n",
    "    \n",
    "        # Set weighting of Dual-Guidance \n",
    "        text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "        for name, module in vd_pipe.image_unet.named_modules():\n",
    "            if isinstance(module, DualTransformer2DModel):\n",
    "                module.mix_ratio = text_image_ratio\n",
    "                for i, type in enumerate((\"text\", \"image\")):\n",
    "                    if type == \"text\":\n",
    "                        module.condition_lengths[i] = 77\n",
    "                        module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "                    else:\n",
    "                        module.condition_lengths[i] = 257\n",
    "                        module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "                        \n",
    "        unet = vd_pipe.image_unet\n",
    "        vae = vd_pipe.vae\n",
    "        noise_scheduler = vd_pipe.scheduler\n",
    "        # config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        # config = OmegaConf.to_container(config, resolve=True)\n",
    "        # unclip_params = config[\"model\"][\"params\"]\n",
    "        # network_config = unclip_params[\"network_config\"]\n",
    "        # denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        # first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        # conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        # sampler_config = unclip_params[\"sampler_config\"]\n",
    "        # scale_factor = unclip_params[\"scale_factor\"]\n",
    "        # disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        # offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        # first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        # sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        # diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "        #                        denoiser_config=denoiser_config,\n",
    "        #                        first_stage_config=first_stage_config,\n",
    "        #                        conditioner_config=conditioner_config,\n",
    "        #                        sampler_config=sampler_config,\n",
    "        #                        scale_factor=scale_factor,\n",
    "        #                        disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # # set to inference\n",
    "        # diffusion_engine.eval().requires_grad_(False)\n",
    "        # diffusion_engine.to(device)\n",
    "\n",
    "        # ckpt_path = '/weka/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "        # ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        # diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        # image = images[:1].to(device)\n",
    "        # batch={\"jpg\": image,\n",
    "        #       \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * 768,\n",
    "        #       \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        # out = diffusion_engine.conditioner(batch)\n",
    "        # vector_suffix = out[\"vector\"].to(device)\n",
    "        # print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef7884db-6641-4654-9f2c-db695aebe828",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_git = False\n",
    "if use_git:\n",
    "    from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "    from modeling_git import GitForCausalLMClipEmb\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "    clip_text_model.to(device)\n",
    "    clip_text_model.eval().requires_grad_(False)\n",
    "\n",
    "    pixel_values = processor(images=images[:1], return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n",
    "    image_features = clip_text_model.git.image_encoder(pixel_values).last_hidden_state\n",
    "    _, clip_text_seq_dim, clip_text_emb_dim = image_features.shape\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_text_dim = clip_text_emb_dim\n",
    "    depth = 3\n",
    "    dim_head = 32\n",
    "    heads = clip_text_emb_dim//32 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 50\n",
    "    git_prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_text_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_text_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "    model.git_diffusion_prior = BrainDiffusionPrior(\n",
    "        net=git_prior_network,\n",
    "        image_embed_dim=out_text_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "\n",
    "    utils.count_params(model.git_diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 1120\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "1,083,291,150 total\n",
      "1,083,291,134 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "if use_git:\n",
    "    opt_grouped_parameters.extend([\n",
    "            {'params': [p for n, p in model.git_diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "            {'params': [p for n, p in model.git_diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    if use_deepspeed:\n",
    "        deepspeed.DeepSpeedEngine.save_checkpoint(model, save_dir=outdir, tag=tag)\n",
    "        ckpt_path = outdir+f'/{tag}/{tag}.npy'\n",
    "        np.save(ckpt_path, {\n",
    "            'epoch': epoch,\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs})\n",
    "    else:\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "        del unwrapped_model\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,use_deepspeed=use_deepspeed,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    if use_deepspeed:\n",
    "        state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "        if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "            state_dict.pop('ridge.linears.0.weight',None)\n",
    "        try:\n",
    "            model.module.load_state_dict(state_dict, strict=strict)\n",
    "        except:\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "        if load_epoch:\n",
    "            np_ckpt = np.load(outdir+f'/{tag}/{tag}.npy', allow_pickle=True).tolist()\n",
    "            globals()[\"epoch\"] = np_ckpt['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "        if load_optimizer or load_lr:\n",
    "            print(\"cant load optimizer or lr when using deepspeed.\")\n",
    "    else:\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "            state_dict.pop('ridge.linears.0.weight',None)\n",
    "        try:\n",
    "            model.module.load_state_dict(state_dict, strict=strict)\n",
    "        except:\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "        if load_epoch:\n",
    "            globals()[\"epoch\"] = checkpoint['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "        if load_optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if load_lr:\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        del checkpoint\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb mindeye2_vd run meV2_ablation\n",
      "wandb_config:\n",
      " {'model_name': 'meV2_ablation', 'global_batch_size': 32, 'batch_size': 32, 'num_epochs': 16, 'num_sessions': 3, 'num_params': 1083291134, 'clip_scale': 1.0, 'prior_scale': 30.0, 'blur_scale': 0.5, 'git_scale': 3.0, 'use_image_aug': False, 'max_lr': 0.0003, 'mixup_pct': 0.33, 'num_samples_per_epoch': 2250, 'num_test': 3000, 'ckpt_interval': 1, 'ckpt_saving': False, 'seed': 42, 'distributed': True, 'num_devices': 1, 'world_size': 1, 'train_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar', 'test_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_test/0.tar'}\n",
      "wandb_id: meV2_ablation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihirneal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/mihirneal/MindEyeV2/src/wandb/run-20240118_124013-3irt32qx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://stability.wandb.io/mihirneal/mindeye2_vd/runs/3irt32qx\" target=\"_blank\">meV2_ablation</a></strong> to <a href=\"https://stability.wandb.io/mihirneal/mindeye2_vd\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye2_vd'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"git_scale\": git_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None and not resume_from_ckpt:\n",
    "    # if multisubject_ckpt was saved using deepspeed but currently you are not, you'll need to import deepspeed and manually set use_deepspeed to True in the load_ckpt function\n",
    "    # import deepspeed\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,use_deepspeed=use_deepspeed,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /admin/home-mihirneal/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /admin/home-mihirneal/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.943218231201172 seconds\n",
      "[2024-01-18 12:40:30,339] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:40:33,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-18 12:40:33,192] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-18 12:40:33,192] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-01-18 12:40:33,199] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-01-18 12:40:33,200] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-01-18 12:40:33,200] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2024-01-18 12:40:33,201] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 10000000\n",
      "[2024-01-18 12:40:33,201] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\n",
      "[2024-01-18 12:40:33,201] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-01-18 12:40:33,202] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-01-18 12:40:39,260] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-18 12:40:39,261] [INFO] [utils.py:792:see_memory_usage] MA 7.15 GB         Max_MA 7.15 GB         CA 7.19 GB         Max_CA 7 GB \n",
      "[2024-01-18 12:40:39,262] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 96.19 GB, percent = 8.6%\n",
      "[2024-01-18 12:40:42,834] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-01-18 12:40:42,836] [INFO] [utils.py:792:see_memory_usage] MA 7.15 GB         Max_MA 7.15 GB         CA 7.19 GB         Max_CA 7 GB \n",
      "[2024-01-18 12:40:42,836] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 108.45 GB, percent = 9.7%\n",
      "[2024-01-18 12:40:42,837] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-01-18 12:40:43,039] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-01-18 12:40:43,040] [INFO] [utils.py:792:see_memory_usage] MA 7.15 GB         Max_MA 7.15 GB         CA 7.19 GB         Max_CA 7 GB \n",
      "[2024-01-18 12:40:43,041] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 108.6 GB, percent = 9.7%\n",
      "[2024-01-18 12:40:43,048] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
      "[2024-01-18 12:40:43,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-01-18 12:40:43,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.OneCycleLR object at 0x7f4d06270760>\n",
      "[2024-01-18 12:40:43,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05], mom=[(0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999)]\n",
      "[2024-01-18 12:40:43,050] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-01-18 12:40:43,051] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-01-18 12:40:43,051] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-01-18 12:40:43,052] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-01-18 12:40:43,052] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-01-18 12:40:43,052] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-01-18 12:40:43,053] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-01-18 12:40:43,053] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-01-18 12:40:43,053] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-01-18 12:40:43,053] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-01-18 12:40:43,054] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4c08b7cb80>\n",
      "[2024-01-18 12:40:43,054] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-01-18 12:40:43,054] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-01-18 12:40:43,055] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-01-18 12:40:43,055] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-01-18 12:40:43,055] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-01-18 12:40:43,055] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-01-18 12:40:43,056] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-01-18 12:40:43,056] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-01-18 12:40:43,056] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-01-18 12:40:43,056] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-01-18 12:40:43,057] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-01-18 12:40:43,057] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-01-18 12:40:43,057] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-01-18 12:40:43,058] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-01-18 12:40:43,058] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-01-18 12:40:43,058] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-01-18 12:40:43,058] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-01-18 12:40:43,059] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-01-18 12:40:43,059] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-01-18 12:40:43,059] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-01-18 12:40:43,060] [INFO] [config.py:988:print]   fp16_auto_cast ............... False\n",
      "[2024-01-18 12:40:43,060] [INFO] [config.py:988:print]   fp16_enabled ................. True\n",
      "[2024-01-18 12:40:43,060] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-01-18 12:40:43,060] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-01-18 12:40:43,061] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-01-18 12:40:43,061] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-01-18 12:40:43,061] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-01-18 12:40:43,061] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-01-18 12:40:43,062] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-01-18 12:40:43,062] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-01-18 12:40:43,062] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-01-18 12:40:43,062] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-01-18 12:40:43,063] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-01-18 12:40:43,063] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-01-18 12:40:43,063] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-01-18 12:40:43,064] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-01-18 12:40:43,064] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-01-18 12:40:43,064] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-01-18 12:40:43,064] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-01-18 12:40:43,065] [INFO] [config.py:988:print]   optimizer_name ............... None\n",
      "[2024-01-18 12:40:43,065] [INFO] [config.py:988:print]   optimizer_params ............. None\n",
      "[2024-01-18 12:40:43,065] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-01-18 12:40:43,066] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-01-18 12:40:43,066] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-01-18 12:40:43,066] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-01-18 12:40:43,066] [INFO] [config.py:988:print]   scheduler_name ............... None\n",
      "[2024-01-18 12:40:43,067] [INFO] [config.py:988:print]   scheduler_params ............. None\n",
      "[2024-01-18 12:40:43,067] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-01-18 12:40:43,067] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-01-18 12:40:43,067] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-01-18 12:40:43,068] [INFO] [config.py:988:print]   steps_per_print .............. inf\n",
      "[2024-01-18 12:40:43,068] [INFO] [config.py:988:print]   train_batch_size ............. 32\n",
      "[2024-01-18 12:40:43,068] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-01-18 12:40:43,068] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-01-18 12:40:43,069] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-01-18 12:40:43,069] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-01-18 12:40:43,069] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-01-18 12:40:43,070] [INFO] [config.py:988:print]   world_size ................... 1\n",
      "[2024-01-18 12:40:43,070] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True\n",
      "[2024-01-18 12:40:43,070] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=10000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=PosixPath('/scratch'), buffer_count=5, buffer_size=4000000000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/scratch'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=10000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-01-18 12:40:43,070] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-01-18 12:40:43,071] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-01-18 12:40:43,071] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-01-18 12:40:43,071] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_prefetch_bucket_size\": 1.000000e+07, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+05, \n",
      "        \"reduce_bucket_size\": 1.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"buffer_size\": 4.000000e+09, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 2.621440e+07, \n",
      "        \"queue_depth\": 32, \n",
      "        \"thread_count\": 1, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meV2_ablation starting with epoch 0 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:41:05,009] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 12:41:05,940] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2024-01-18 12:41:06,870] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2024-01-18 12:41:07,805] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2024-01-18 12:41:08,734] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2024-01-18 12:41:09,664] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2024-01-18 12:41:10,593] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2024-01-18 12:41:11,522] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2024-01-18 12:41:12,452] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2024-01-18 12:41:13,382] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2024-01-18 12:41:14,316] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2024-01-18 12:41:15,246] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2024-01-18 12:41:16,177] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2024-01-18 12:41:17,109] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2024-01-18 12:41:18,039] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2024-01-18 12:41:18,971] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2024-01-18 12:41:19,900] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2024-01-18 12:41:20,829] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-01-18 12:41:21,757] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "---\n",
      "reconstructing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n",
      "using existing embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%| | 1/16 [08:12<2:03:07, 492.48s/it, test/blur_recons=<wandb.sdk.data_types.image.Image object at 0x7f4bf8179fc0>, test/blurry_pixcorr=0.148, test/loss=5.42, test/loss_clip_total=5.24, test/loss_prior=0.00608, test/num_steps=1, test/orig=[<wandb.sdk.data_types.image.Image object at 0x7f4c2d017ee0>, <wandb.sdk.data_types.image.Image object at 0x7f4bf8178760>, <wandb.sdk.data_types.image.Image object at 0x7f4bf8179e70>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebe710>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebd030>], test/recon_cossim=0.0559, test/recon_mse=0.00273, test/recons=[<wandb.sdk.data_types.image.Image object at 0x7f4bf8179ff0>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebe7a0>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebf6d0>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebefb0>, <wandb.sdk.data_types.image.Image object at 0x7f4c2cebf850>], test/test_bwd_pct_correct=0.0167, test/test_fwd_pct_correct=0.05, train/blurry_pixcorr=0.116, train/bwd_pct_correct=0.107, train/fwd_pct_correct=0.203, train/loss=8.47, train/loss_blurry_cont_total=7.37, train/loss_blurry_total=0.645, train/loss_clip_total=3.2, train/loss_prior=0.1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.02 GiB. GPU 0 has a total capacty of 39.56 GiB of which 3.31 GiB is free. Including non-PyTorch memory, this process has 36.24 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 13.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 205\u001b[0m\n\u001b[1;32m    202\u001b[0m         blurry_pixcorr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixcorr\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    204\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_loss(loss)\n\u001b[0;32m--> 205\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    208\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/accelerate/accelerator.py:1899\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_engine_wrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mMEGATRON_LM:\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/accelerate/utils/deepspeed.py:167\u001b[0m, in \u001b[0;36mDeepSpeedEngineWrapper.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# runs backpropagation and handles mixed precision\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Deepspeed's `engine.step` performs the following operations:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# - gradient accumulation check\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# - gradient clipping\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# - checking overflow\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# - lr_scheduler step (only if engine.lr_scheduler is not None)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:15\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     14\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_push(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_pop()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_val\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/engine.py:1955\u001b[0m, in \u001b[0;36mDeepSpeedEngine.backward\u001b[0;34m(self, loss, allreduce_gradients, release_loss, retain_graph, scale_wrt_gas)\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_optimization():\n\u001b[1;32m   1954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary()\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_enabled():\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;66;03m# AMP requires delaying unscale when inside gradient accumulation boundaries\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     delay_unscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary()\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:2019\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.backward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m   2017\u001b[0m     scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2019\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;66;03m# Only for Stage 1, Mode 2\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_grad_accum_attribute:\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py:63\u001b[0m, in \u001b[0;36mLossScalerBase.backward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     scaled_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_scale\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mscaled_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:865\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.create_reduce_and_remove_grad_hooks.<locals>.wrapper.<locals>.reduce_partition_and_remove_grads\u001b[0;34m(*notneeded)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_partition_and_remove_grads\u001b[39m(\u001b[38;5;241m*\u001b[39mnotneeded):\n\u001b[0;32m--> 865\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_ready_partitions_and_remove_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1377\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.reduce_ready_partitions_and_remove_grads\u001b[0;34m(self, param, i)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_ready_partitions_and_remove_grads\u001b[39m(\u001b[38;5;28mself\u001b[39m, param, i):\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartition_gradients \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_independent_p_g_buckets_and_remove_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:893\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.reduce_independent_p_g_buckets_and_remove_grads\u001b[0;34m(self, param, i)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melements_in_ipg_bucket \u001b[38;5;241m+\u001b[39m param\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_bucket_size:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_ipg_memory_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn ipg_remove_grads before reduce_ipg_grads\u001b[39m\u001b[38;5;124m\"\u001b[39m, param\u001b[38;5;241m.\u001b[39mnumel())\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_ipg_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous_gradients \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverlap_comm:\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;66;03m# Swap ipg_index between 0 and 1\u001b[39;00m\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipg_index\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1364\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.reduce_ipg_grads\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_grad_attribute(param)\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous_gradients:\n\u001b[0;32m-> 1364\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_grads_in_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# zero stage 1 - partition only optimizer state\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontiguous_gradients \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_param_in_current_partition[param_id]:\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1288\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.copy_grads_in_partition\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_accumulate_grad_in_cpu_via_gpu(param)\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary:\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_norm_for_param_grad_in_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_overflow_tracker_for_param_grad(param)\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_inplace_copy_grad_to_fp32_buffer_from_gpu(param)\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1218\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.set_norm_for_param_grad_in_gpu\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m   1215\u001b[0m start \u001b[38;5;241m=\u001b[39m source_offset\n\u001b[1;32m   1216\u001b[0m accumulated_grad \u001b[38;5;241m=\u001b[39m accumulated_grad\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnarrow(\u001b[38;5;241m0\u001b[39m, start, num_elements)\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_for_param_grads[param_id] \u001b[38;5;241m=\u001b[39m \u001b[43maccumulated_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.02 GiB. GPU 0 has a total capacty of 39.56 GiB of which 3.31 GiB is free. Including non-PyTorch memory, this process has 36.24 GiB memory in use. Of the allocated memory 20.73 GiB is allocated by PyTorch, and 13.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "visualize_prior = True\n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):    \n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                voxel0 = torch.Tensor(voxel0)\n",
    "                \n",
    "                if seq_len==1:\n",
    "                    voxel0 = voxel0.unsqueeze(1)\n",
    "                else:\n",
    "                    if seq_past>0:\n",
    "                        past_behavior = past_behav0[:,:(seq_past),5].cpu().long()\n",
    "                        past_voxel0 = voxels[f'subj0{subj_list[s]}'][past_behavior]\n",
    "                        past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                        past_voxel0 = torch.Tensor(past_voxel0)\n",
    "\n",
    "                        # if shared1000, then you need to mask it out \n",
    "                        for p in range(seq_past):\n",
    "                            mask = (past_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                            index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                            past_voxel0[index,p,:] = torch.zeros_like(past_voxel0[index,p,:])\n",
    "\n",
    "                    if seq_future>0:\n",
    "                        future_behavior = future_behav0[:,:(seq_future),5].cpu().long()\n",
    "                        future_voxel0 = voxels[f'subj0{subj_list[s]}'][future_behavior]\n",
    "                        future_voxel0[future_behavior==-1] = voxel0[torch.where(future_behavior==-1)[0]] # replace invalid past voxels \n",
    "                        future_voxel0 = torch.Tensor(future_voxel0)\n",
    "\n",
    "                        # if shared1000, then you need to mask it out \n",
    "                        for p in range(seq_future):\n",
    "                            mask = (future_behav0[:,p,-1] == 1) # [16,] bool\n",
    "                            index = torch.nonzero(mask.cpu()).squeeze()\n",
    "                            future_voxel0[index,p,:] = torch.zeros_like(future_voxel0[index,p,:])\n",
    "\n",
    "                    # concatenate current timepoint with past/future\n",
    "                    if seq_past > 0 and seq_future > 0:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                        voxel0 = torch.cat((voxel0, future_voxel0), axis=1)\n",
    "                    elif seq_past > 0:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                    else:\n",
    "                        voxel0 = torch.cat((voxel0.unsqueeze(1), future_voxel0), axis=1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "\n",
    "            voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            if use_git:\n",
    "                pixel_values = processor(images=image, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n",
    "            image = image.to(device)\n",
    "\n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            clip_target = clip_extractor.embed_image(image).float()\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "            voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "            backbone, clip_voxels, blurry_image_enc_, text_ = model.backbone(voxel_ridge)\n",
    "\n",
    "            if clip_scale>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if use_git:\n",
    "                text_features = clip_text_model.git.image_encoder(pixel_values).last_hidden_state\n",
    "                loss_prior, text_prior_out = model.git_diffusion_prior(text_embed=text_, image_embed=text_features)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= git_scale\n",
    "                loss += loss_prior\n",
    "\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):                \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "\n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "\n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "\n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                    \n",
    "                    if seq_len==1:\n",
    "                        voxel = voxel.unsqueeze(1)\n",
    "                    else:\n",
    "                        if seq_past>0:\n",
    "                            past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                            past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                            if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                        if seq_future>0:\n",
    "                            future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                            future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                            if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                                future_voxels[torch.where(future_behavior==-1)[0]] = 0\n",
    "                            \n",
    "                        if seq_past > 0 and seq_future > 0:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                            voxel = torch.cat((voxel, future_voxels), axis=1)\n",
    "                        elif seq_past > 0:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                        else:\n",
    "                            voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "\n",
    "                loss=0.\n",
    "                            \n",
    "                test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
    "                assert len(image) == 300\n",
    "\n",
    "                clip_target = clip_extractor.embed_image(image).float()\n",
    "\n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep],0) # 0th index of subj_list\n",
    "                    backbone0, clip_voxels0, blurry_image_enc_, _ = model.backbone(voxel_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    \n",
    "                    if visualize_prior:\n",
    "                        # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                        prior_out = model.diffusion_prior.p_sample_loop(backbone[random_samps].shape, \n",
    "                                        text_cond = dict(text_embed = backbone[random_samps]), \n",
    "                                        cond_scale = 1., timesteps = timesteps)\n",
    "\n",
    "                        test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target[random_samps]).mean().item()\n",
    "                        test_recon_mse += mse(prior_out, clip_target[random_samps]).item()\n",
    "                        \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = np.random.randint(0, 3)\n",
    "                    print(f\"reconstructing...\")\n",
    "                    vd_pipe = vd_pipe.to(device)\n",
    "                    recon_grid, img_grid = utils.reconstruction(\n",
    "                        backbone_emb=prior_out,\n",
    "                        imgs=image,\n",
    "                        vd_pipe=vd_pipe\n",
    "                    )\n",
    "                    # samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                    #          diffusion_engine,\n",
    "                    #          vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        \n",
    "                        logs[f\"test/orig\"] = [wandb.Image(img_grid[i], caption=f\"epoch{epoch:03d}\") for i in range(5)]\n",
    "                        logs[f\"test/recons\"] = [wandb.Image(recon_grid[i], caption=f\"epoch{epoch:03d}\") for i in range(5)]\n",
    "                    # if utils.is_interactive():\n",
    "                    #     plt.figure(figsize=(2,2))\n",
    "                    #     plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                    #     plt.axis('off')\n",
    "                    #     plt.show()\n",
    "                        \n",
    "                    #     plt.figure(figsize=(2,2))\n",
    "                    #     plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                    #     plt.axis('off')\n",
    "                    #     plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c391fa-53d6-4dda-9558-19df4b4798ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46075e50-dde5-497b-a174-fd6426cff274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c9ce9-7c45-448d-8794-22db0dbf1b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b71aa-713c-4ff7-ba6d-32a4042d3caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fedd60-1f26-4f12-9a30-54b8775e04dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
