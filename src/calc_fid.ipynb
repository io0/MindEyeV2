{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a095dda6-8145-493f-a6f1-3af610fe5660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from /weka/proj-fmri/shared/coco/sampled_imgs: 100%|█████████████████████████████████████████████████████████████████████████████████| 30000/30000 [04:36<00:00, 108.42it/s]\n",
      "Loading images from /weka/proj-fmri/shared/coco/vd_imgs: 100%|███████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [56:04<00:00,  8.92it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 29.97 GiB. GPU 0 has a total capacty of 39.56 GiB of which 9.07 GiB is free. Including non-PyTorch memory, this process has 30.49 GiB memory in use. Of the allocated memory 30.07 GiB is allocated by PyTorch, and 21.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[1;32m     40\u001b[0m real_images \u001b[38;5;241m=\u001b[39m load_images_from_directory(real_images_dir)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_images_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for efficiency\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 29.97 GiB. GPU 0 has a total capacty of 39.56 GiB of which 9.07 GiB is free. Including non-PyTorch memory, this process has 30.49 GiB memory in use. Of the allocated memory 30.07 GiB is allocated by PyTorch, and 21.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.models import inception_v3\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Function to load and preprocess images from a directory\n",
    "def load_images_from_directory(directory, expected_count=30000):\n",
    "    images = []\n",
    "    image_count = 0\n",
    "\n",
    "    for filename in tqdm(os.listdir(directory), desc=f\"Loading images from {directory}\"):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Check for image files\n",
    "            image_count += 1\n",
    "            img = Image.open(os.path.join(directory, filename)).convert('RGB')\n",
    "            img = TF.resize(img, [299, 299])  # Resize image\n",
    "            img = TF.to_tensor(img)  # Convert to tensor\n",
    "            img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "            images.append(img.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "    if image_count != expected_count:\n",
    "        raise ValueError(f\"Expected {expected_count} images in directory '{directory}', but found {image_count} images.\")\n",
    "\n",
    "    return torch.cat(images, dim=0)  # Concatenate all images into a single tensor\n",
    "\n",
    "# Load the Inception V3 model\n",
    "model = inception_v3(pretrained=True)\n",
    "model.fc = torch.nn.Identity()  # Remove the final fully connected layer\n",
    "model.eval().to(device)\n",
    "\n",
    "# Directories containing your images\n",
    "real_images_dir = '/weka/proj-fmri/shared/coco/sampled_imgs'\n",
    "generated_images_dir = '/weka/proj-fmri/shared/coco/vd_imgs'\n",
    "\n",
    "# Load images\n",
    "real_images = load_images_from_directory(real_images_dir).to(device)\n",
    "generated_images = load_images_from_directory(generated_images_dir).to(device)\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    real_features = model(real_images)\n",
    "    generated_features = model(generated_images)\n",
    "\n",
    "# Initialize FID\n",
    "fid = FrechetInceptionDistance(feature=2048)\n",
    "\n",
    "# Update state with real and generated features\n",
    "fid.update(real_features, real=True)\n",
    "fid.update(generated_features, real=False)\n",
    "\n",
    "# Compute FID score\n",
    "fid_score = fid.compute()\n",
    "print(f'FID score: {fid_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b16433f-3cb3-41db-b1db-b5032f0b2fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pip list has been saved to pip_list.yaml\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "# Run 'pip list' command and capture the output\n",
    "pip_list_output = subprocess.check_output([\"pip\", \"list\"]).decode(\"utf-8\")\n",
    "\n",
    "# Split the output into lines and ignore the header\n",
    "package_lines = pip_list_output.strip().split('\\n')[2:]\n",
    "\n",
    "# Create a list of dictionaries for each package\n",
    "packages = []\n",
    "for line in package_lines:\n",
    "    package_info = line.strip().split()\n",
    "    package_name = package_info[0]\n",
    "    package_version = package_info[1]\n",
    "    packages.append({\"package\": package_name, \"version\": package_version})\n",
    "\n",
    "# Define the path for the output YAML file\n",
    "output_yaml_file = \"pip_list.yaml\"\n",
    "\n",
    "# Write the package information to the YAML file\n",
    "with open(output_yaml_file, \"w\") as yaml_file:\n",
    "    yaml.dump(packages, yaml_file, default_flow_style=False)\n",
    "\n",
    "print(f\"Pip list has been saved to {output_yaml_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7c9ba-30a8-4c7e-b536-2e825458a605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
